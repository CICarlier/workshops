
    




    
\documentclass[11pt]{article}

    
    \usepackage[breakable]{tcolorbox}
    \tcbset{nobeforeafter} % prevents tcolorboxes being placing in paragraphs
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{nhanes\_ols}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \newcommand{\prompt}[4]{
        \llap{{\color{#2}[#3]: #4}}\vspace{-1.25em}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{basic-regression-using-python-and-statsmodels-a-case-study-using-the-nhanes-data}{%
\section{Basic regression using Python and Statsmodels -- a case study
using the NHANES
data}\label{basic-regression-using-python-and-statsmodels-a-case-study-using-the-nhanes-data}}

    This notebook introduces some of the main ideas of regression analysis,
focusing on the practical aspects of fitting regression models in Python
using the \href{http://www.statsmodels.org}{Statsmodels} package. We
will also be using the \href{http://pandas.pydata.org}{Pandas} library
for data management, and the \href{http://www.numpy.org}{Numpy} library
for numerical calculations.

    At a high level, regression analysis is a way of relating \emph{inputs}
to \emph{outputs}. For example, if we want to understand how the fuel
efficiency of a car, measured in gallons per mile, relates to the
vehicle's weight and engine displacement, then vehicle weight and engine
displacement could be taken as the inputs, with fuel efficiency as the
output. Note that this does not mean that we view vehicle weight and
engine displacement as being the only direct determinants of fuel
efficiency. For example, the cross sectional area of the vehicle would
also matter a lot (for aerodynamics). Regression modeling can be used
when we have one or more input factors that may partially explain the
output, allowing us to estimate how the inputs and outputs are
statistically related.

    Regression analysis has a long history, with roots in many different
fields. As a result, there are multiple terms for many of the key
concepts. For example, the ``output'' discussed above is often called
the \emph{dependent variable}, the \emph{outcome}, or the
\emph{response}. The ``inputs'' discussed above are often called the
\emph{covariates}, \emph{regressors}, or \emph{independent variables}.

    \hypertarget{regression-analysis-and-conditional-distributions}{%
\section{Regression analysis and conditional
distributions}\label{regression-analysis-and-conditional-distributions}}

    A more formal way of looking at regression analysis is through the idea
of a \emph{conditional distribution}. Switching to another example let
\(y\) denote a person's blood pressure, and let \(a\) and \(g\) denote
their age and gender, respectively. Our data represent a population, for
example, all adults in the United States. There is a ``marginal mean''
of all blood pressure values in this population, which can be estimated
using the sample mean (average value) of all the blood pressure values
in our data set. There are also many different ``conditional means'',
which capture the mean blood pressure for the subset of all adults in
the US having a particular age and gender.

It is common to use the term ``expectation'' to refer to a mean or
average value in a population, and denote it by the letter ``E''. Thus,
\(E[y]\) is the (marginal) expected blood pressure for all US adults,
and \(E[y \;|\; a, g]\) is the conditional mean blood pressure for all
adults of a particular age and gender. For example,
\(E[y \;|\; a=35, g=F]\) is the conditional mean blood pressure for all
35 year-old women in our population.

    Directly analogous to conditional means is the concept of ``conditional
variance''. The marginal variance \(Var[y]\) is the variance (expected
squared deviation from the mean) of the whole population. The
conditional variance \(Var[y \;|\; a=35, g=F]\) is the variance of \(y\)
for the subpopulation of 35 year-old women.

    As with most forms of statistical analysis, the goal of regression is to
use the sample to learn about the population, and to quantify the
uncertainty in doing so. Since we have a finite sample of data, we are
not able to recover \(E[y \;|\; a,g]\) directly, but rather will only be
able to estimate it. We can then use the tools of statistical inference
(e.g.~standard errors, confidence intervals, and hypothesis tests) to
assess how precisely we have recovered the ``estimation target''
\(E[y \;|\; a, g]\).

    \hypertarget{nhanes}{%
\section{NHANES}\label{nhanes}}

    The National Health and Nutrition Examination Study (NHANES) is a study
sponsored by the US government that has run in waves since the 1970's.
Each NHANES wave is a cross sectional study of around 10,000 people,
selected to represent the US adult population at a particular point in
time. A large number of measures are made on each of the NHANES
subjects. Here we will focus on basic demographic characteristics, body
dimensions (e.g.~height or BMI), and blood pressure (systolic and
diastolic, measured three times each).

    Note that the NHANES data were collected as a designed survey, and in
general should be analyzed as such. This means that survey design
information such as sampling weights, strata, and clusters should be
accounted for in any analysis using NHANES. But to introduce how linear
regression is most commonly used, we will not incorporate the survey
structure of the NHANES sample into the analyses conducted here.

    We begin by importing the libraries that we will be using.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/nfs/kshedden/python3/lib/python3.7/site-
packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is
removed from pandas. Accessing it from the top-level namespace will also be
removed in the next version
  data\_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)
\end{Verbatim}

    Next we will load the data. The NHANES study encompasses multiple waves
of data collection. Here we will only use the 2015-2016 data. As with
most data sets, there are some missing values in the NHANES files. While
many of the methods demonstrated below would handle missing values
automatically (at least in a crude way), here we drop up-front all rows
with missing values in any of the key variables that we will use in this
notebook.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{url} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{https://raw.githubusercontent.com/kshedden/statswpy/master/NHANES/merged/nhanes\PYZus{}2015\PYZus{}2016.csv}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{da} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{url}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Drop unused columns, drop rows with any missing values.}
\PY{n+nb}{vars} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDRETH1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DMDEDUC2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SMQ020}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{da} \PY{o}{=} \PY{n}{da}\PY{p}{[}\PY{n+nb}{vars}\PY{p}{]}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{linear-regression-and-least-squares}{%
\section{Linear regression and least
squares}\label{linear-regression-and-least-squares}}

    The \emph{regression function} \(E[y | x]\) (for one or more covariates
\(x\)) could in principle be any function of \(x\), i.e.~a mathematical
function mapping vectors \(x\) to real numbers \(y\). To make the
analysis more tractable, many forms of regression analysis estimate
\(E[y | x]\) by modeling it, often in a parametric form. Specifically,
we may use the \emph{linear mean structure}

\[
E[y | x] = b_0 + b_1x_1 + \cdots + b_px_p.
\]

    This mean structure implies that when comparing two observations whose
values of a covariate, say \(x_j\), differ by one unit, and that are
identical in terms of all other covarates, then the expected values of
the dependent variable for these two observations will differ by \(b_j\)
units.

    Note that using a linear mean structure does not imply that we are
limited to modeling linear phenomena. The covariates \(x_j\) can
themselves be nonlinear functions of other covariates, allowing us to
capture many nonlinear relationships using linear regression. We will
discuss this point in more detail below.

    The most widely-used approach for fitting models with linear mean
structures to observed data sets is \emph{least squares}. We will not
discuss the theory of least squares analysis here further, except to
note that it has a number of good properties, including being fast to
compute, and allowing for relatively straightforward statistical
inference. Although there are some settings where least squares is known
to perform poorly, in many commonly-encountered settings it should give
meaningful results and be competitive in performance with any other
approach.

    \hypertarget{regression-analysis-with-the-nhanes-data}{%
\section{Regression analysis with the NHANES
data}\label{regression-analysis-with-the-nhanes-data}}

    We will focus initially on regression models in which systolic
\href{https://en.wikipedia.org/wiki/Blood_pressure}{blood pressure}
(SBP) is the outcome (dependent) variable. That is, we will predict SBP
from other variables. SBP is an important indicator of cardiovascular
health. It tends to increase with age, is greater for overweight people
(i.e.~people with greater body mass index or BMI), and also differs
among demographic groups, for example among gender and ethnic groups.

    Since SBP is a quantitative variable, we will model it using least
squares to fit a regression model with a linear mean structure. This is
commonly referred to as simply ``linear regression''. While linear
regression is commonly used with quantitative outcome variables, it is
not the only regression method that can be used with quantitative
outcomes, nor is it the case that linear regression can only be used
with quantitative outcomes. However, linear regression is a good default
starting point for any regression analysis involving a quantitative
outcome variable.

    \hypertarget{interpreting-regression-parameters-in-a-basic-model}{%
\subsection{Interpreting regression parameters in a basic
model}\label{interpreting-regression-parameters-in-a-basic-model}}

    We start with a simple linear regression model with only one covariate,
age, predicting SBP. In the NHANES data, the variable
\href{https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BPX_I.htm\#BPXSY1}{BPXSY1}
contains the first recorded measurement of SBP for a subject, and
\href{https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm\#RIDAGEYR}{RIDAGEYR}
is the subject's age in years. The model that is fit in the next code
cell expresses the expected SBP as a linear function of age:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.207
Model:                            OLS   Adj. R-squared:                  0.207
Method:                 Least Squares   F-statistic:                     1333.
Date:                Mon, 27 Jan 2020   Prob (F-statistic):          2.09e-259
Time:                        15:01:14   Log-Likelihood:                -21530.
No. Observations:                5102   AIC:                         4.306e+04
Df Residuals:                    5100   BIC:                         4.308e+04
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    102.0935      0.685    149.120      0.000     100.751     103.436
RIDAGEYR       0.4759      0.013     36.504      0.000       0.450       0.501
==============================================================================
Omnibus:                      690.261   Durbin-Watson:                   2.039
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1505.999
Skew:                           0.810   Prob(JB):                         0.00
Kurtosis:                       5.112   Cond. No.                         156.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
\end{Verbatim}

    Much of the output above is not relevant for us, so focus on the center
section of the output where the header row begins with \textbf{coef}.
This section contains the estimated values of the parameters of the
regression model, their standard errors, and other values that are used
to quantify the uncertainty in the regression parameter estimates. Note
that the parameters of a regression model, which appear in the column
labeled \textbf{coef} in the table above, may also be referred to as
\emph{slopes} or \emph{effects}.

    This fitted model implies that when comparing two people whose ages
differ by one year, the older person will on average have 0.48 units
higher SBP than the younger person. This difference is statistically
significant, based on the p-value shown under the column labeled
\textbf{\texttt{P\textgreater{}\textbar{}t\textbar{}}}. This means that
there is strong evidence that there is a real association between
between systolic blood pressure and age in this population.

    SBP is measured in units of \emph{millimeters of mercury}, expressed
\emph{mm/Hg}. In order to better understand the meaning of the estimated
regression parameter 0.48, we can look at the standard deviation of SBP:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{da}\PY{o}{.}\PY{n}{BPXSY1}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, boxrule=.5pt, size=fbox, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{5}{\hspace{3.5pt}}
\begin{Verbatim}[commandchars=\\\{\}]
18.486559500781865
\end{Verbatim}
\end{tcolorbox}
        
    The standard deviation of around 18.5 tells us that a randomly selected
person from the population has SBP that deviates by around 18.5 mm/Hg on
average from the population mean SBP value. Thus, there are around 18.5
standard deviation units of variation in the outcome that we are
studying here. The goal of a regression analysis is to explain this
variation using other known factors.

    The standard deviation of around 18.5 is large compared to the
regression slope of 0.48. However the regression slope corresponds to
the change in average SBP for a single year of age, and this effect
accumulates with age. Comparing a 40 year-old person to a 60 year-old
person, there is a 20 year difference in age, which translates into a
\texttt{20\ *\ 0.48\ =\ 9.6} unit difference in average SBP between
these two people. This difference is around half of one standard
deviation, and would generally be considered to be an important and
meaningful shift.

    Below we visualize the distributions of SBP for the whole population
(i.e.~the marginal distribution), and for the subpopulations of people
who are 40 and 60 years old, respectively. This visualization is based
on a model that has been fit to the data. It may be misleading if the
model do not fit the data well. This is an important topic, but we will
set it aside for now.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sbp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)} \PY{c+c1}{\PYZsh{} Grid of possible blood pressure values}
\PY{n}{mn0} \PY{o}{=} \PY{n}{da}\PY{o}{.}\PY{n}{BPXSY1}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} marginal mean blood pressure}
\PY{n}{sd0} \PY{o}{=} \PY{n}{da}\PY{o}{.}\PY{n}{BPXSY1}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} marginal SD of blood pressure}

\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{distributions} \PY{k}{import} \PY{n}{norm}

\PY{n}{y0} \PY{o}{=} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{sbp}\PY{p}{,} \PY{n}{mn0}\PY{p}{,} \PY{n}{sd0}\PY{p}{)}

\PY{n}{mn1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{params}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{40}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} Conditional mean for a 40 year old person}
\PY{n}{sd1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{scale}\PY{p}{)}
\PY{n}{y1} \PY{o}{=} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{sbp}\PY{p}{,} \PY{n}{mn1}\PY{p}{,} \PY{n}{sd1}\PY{p}{)}

\PY{n}{mn2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{params}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{60}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} Conditional mean for a 60 year old person}
\PY{n}{sd2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{scale}\PY{p}{)}
\PY{n}{y2} \PY{o}{=} \PY{n}{norm}\PY{o}{.}\PY{n}{pdf}\PY{p}{(}\PY{n}{sbp}\PY{p}{,} \PY{n}{mn2}\PY{p}{,} \PY{n}{sd2}\PY{p}{)}

\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{sbp}\PY{p}{,} \PY{n}{y0}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Overall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{sbp}\PY{p}{,} \PY{n}{y1}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{40 year old}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{sbp}\PY{p}{,} \PY{n}{y2}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{60 year old}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SBP (mm/Hg)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Density}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_34_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{r-squared-and-correlation}{%
\subsection{R-squared and correlation}\label{r-squared-and-correlation}}

    In the case of regression with a single independent variable, there is a
very close correspondence between a linear regression analysis and a
Pearson correlation analysis. The primary summary statistic for
assessing the strength of a predictive relationship in a regression
model is the \emph{R-squared}, which is shown to be 0.207 in the
regression output above. This means that 21\% of the variation in SBP is
explained by age. Note that this value is exactly the same as the
squared Pearson correlation coefficient between SBP and age, as shown
below.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cc} \PY{o}{=} \PY{n}{da}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cc}\PY{o}{.}\PY{n}{BPXSY1}\PY{o}{.}\PY{n}{RIDAGEYR}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0.20715459625188243
\end{Verbatim}

    There is a second way to interpret the R-squared, which makes use of the
\emph{fitted values} of the regression. The fitted values are
predictions of the blood pressure for each person in the data set, based
on their covariate values. In this case, the only covariate is age, so
we are predicting each NHANES subject's blood pressure as a function of
their age. If we calculate the Pearson correlation coefficient between
the fitted values from the regression, and the actual SBP values, and
then square this correlation coefficient, we see that we again get the
R-squared from the regression model:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cc} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{corrcoef}\PY{p}{(}\PY{n}{da}\PY{o}{.}\PY{n}{BPXSY1}\PY{p}{,} \PY{n}{result}\PY{o}{.}\PY{n}{fittedvalues}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0.2071545962518695
\end{Verbatim}

    Thus, we see that in a linear model fit with only one covariate, the
regression R-squared is equal to the squared Pearson correlation between
the covariate and the outcome, and is also equal to the squared Pearson
correlation between the fitted values and the outcome.

    \hypertarget{adding-a-second-variable}{%
\subsection{Adding a second variable}\label{adding-a-second-variable}}

    Above we considered a simple linear regression analysis with only one
covariate (age) predicting systolic blood pressure (SBP). The real power
of regression analysis arises when we have more than one covariate
predicting an outcome. As noted above, SBP is expected to be related to
gender as well as to age, so we next add gender to the model. The NHANES
variable for gender is named
\href{https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DEMO_I.htm\#RIAGENDR}{RIAGENDR}.

    We begin by creating a relabeled version of the gender variable:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create a labeled version of the gender variable}
\PY{n}{da}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{da}\PY{o}{.}\PY{n}{RIAGENDR}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Male}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Female}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Now we are ready to fit the linear model:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR + RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.215
Model:                            OLS   Adj. R-squared:                  0.214
Method:                 Least Squares   F-statistic:                     697.4
Date:                Mon, 27 Jan 2020   Prob (F-statistic):          1.87e-268
Time:                        15:01:15   Log-Likelihood:                -21505.
No. Observations:                5102   AIC:                         4.302e+04
Df Residuals:                    5099   BIC:                         4.304e+04
Df Model:                           2
Covariance Type:            nonrobust
================================================================================
=====
                        coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
-----
Intercept           100.6305      0.712    141.257      0.000      99.234
102.027
RIAGENDRx[T.Male]     3.2322      0.459      7.040      0.000       2.332
4.132
RIDAGEYR              0.4739      0.013     36.518      0.000       0.448
0.499
==============================================================================
Omnibus:                      706.732   Durbin-Watson:                   2.036
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1582.730
Skew:                           0.818   Prob(JB):                         0.00
Kurtosis:                       5.184   Cond. No.                         168.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
\end{Verbatim}

    The syntax \texttt{RIDAGEYR\ +\ RIAGENDRx} in the cell above does not
mean that these two variables are literally added together. Instead, it
means that these variables are both included in the model as predictors
of blood pressure (\texttt{BPXSY1}).

    The model that was fit above uses both age and gender to explain the
variation in SBP. It finds that two people with the same gender whose
ages differ by one year tend to have blood pressure values differing by
0.47 units, which is essentially the same role for age that we found
above in the model based on age alone. This model also shows us that
comparing a man and a woman of the same age, the man will on average
have 3.23 units greater SBP.

    It is very important to emphasize that the age coefficient of 0.47 is
only meaningful when comparing two people of the same gender, and the
gender coefficient of 3.23 is only meaningful when comparing two people
of the same age. Moreover, these effects are additive, meaning that if
we compare, say, a 50 year old man to a 40 year old woman, the man's
blood pressure will on average be around 3.23 + 10*0.47 = 7.93 units
higher, with the first term in this sum being attributable to gender,
and the second term being attributable to age.

    We noted above that the regression coefficient for age did not change by
much when we added gender to the model. It is important to note however
that in general, the estimated coefficient of a variable in a regression
model will change when other variables are added or removed. We see here
that a coefficient is nearly unchanged if any variables that are added
to or removed from the model are approximately uncorrelated with the
other covariates that are already in the model.

    Below we confirm that gender and age are nearly uncorrelated in this
data set (the correlation of around -0.02 is negligible):

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We need to use the original, numerical version of the gender}
\PY{c+c1}{\PYZsh{} variable to calculate the correlation coefficient.}
\PY{n}{da}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, boxrule=.5pt, size=fbox, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\hspace{3.5pt}}
\begin{Verbatim}[commandchars=\\\{\}]
          RIDAGEYR  RIAGENDR
RIDAGEYR  1.000000 -0.021398
RIAGENDR -0.021398  1.000000
\end{Verbatim}
\end{tcolorbox}
        
    Observe that in the regression output shown above, an R-squared value of
0.215 is listed. Earlier we saw that for a model with only one
covariate, the R-squared from the regression could be defined in two
different ways, either as the squared correlation coefficient between
the covariate and the outcome, or as the squared correlation coefficient
between the fitted values and the outcome. When more than one covariate
is in the model, only the second of these two definitions continues to
hold:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{cc} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{corrcoef}\PY{p}{(}\PY{n}{da}\PY{o}{.}\PY{n}{BPXSY1}\PY{p}{,} \PY{n}{result}\PY{o}{.}\PY{n}{fittedvalues}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{cc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0.21478581086243798
\end{Verbatim}

    \hypertarget{categorical-variables-and-reference-levels}{%
\subsection{Categorical variables and reference
levels}\label{categorical-variables-and-reference-levels}}

    In the model fit above, gender is a categorical variable, and only a
coefficient for males is included in the regression output (i.e.~there
is no coefficient for females in the tables above). Whenever a
categorical variable is used as a covariate in a regression model, one
level of the variable is omitted and is automatically given a
coefficient of zero. This level is called the \emph{reference level} of
the covariate. Here, the female level of the gender variable is the
reference level. This does not mean that being a woman has no impact on
blood pressure. It simply means that we have written the model so that
female blood pressure is the default, and the coefficient for males
(3.23) shifts the blood pressure away from the default.

    We could alternatively have set `male' to be the reference level, in
which case males would be the default, and the female coefficient would
have been around -3.23 (meaning that female blood pressure is 3.23 units
lower than the male blood pressure at a fixed age).

    When using a categorical variable as a predictor in a regression model,
it is recoded into ``dummy variables'' (also known as ``indicator
variables''). A dummy variable for a single level, say \texttt{a}, of a
variable \texttt{x}, is a variable that is equal to \texttt{1} when
\texttt{x=a} and is equal to \texttt{0} when \texttt{x} is not equal to
\texttt{a}. These dummy variables are included in the regression model,
to represent the variable that they are derived from.

    Statsmodels, like most software, will automatically recode a categorical
variable into dummy variables, and will select a reference level (it is
possible to override this choice, but we do not cover that here). When
interpreting the regression output, the level that is omitted should be
seen as having a coefficient of 0, with a standard error of 0. It is
important to note that the selection of a reference level is arbitrary
and does not imply a constraint on the model, or an assumption about the
population that it is intended to capture.

    \hypertarget{a-model-with-three-variables}{%
\subsection{A model with three
variables}\label{a-model-with-three-variables}}

    Next we add a third variable, body mass index (BMI), to the model.
\href{https://en.wikipedia.org/wiki/Body_mass_index}{BMI} is a measure
that is used to assess if a person has healthy weight given their
height.
\href{https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BMX_I.htm\#BMXBMI}{BMXBMI}
is the NHANES variable containing the BMI value for each subject.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR + BMXBMI + RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.228
Model:                            OLS   Adj. R-squared:                  0.228
Method:                 Least Squares   F-statistic:                     502.0
Date:                Mon, 27 Jan 2020   Prob (F-statistic):          8.54e-286
Time:                        15:01:15   Log-Likelihood:                -21461.
No. Observations:                5102   AIC:                         4.293e+04
Df Residuals:                    5098   BIC:                         4.296e+04
Df Model:                           3
Covariance Type:            nonrobust
================================================================================
=====
                        coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
-----
Intercept            91.5840      1.198     76.456      0.000      89.236
93.932
RIAGENDRx[T.Male]     3.5783      0.457      7.833      0.000       2.683
4.474
RIDAGEYR              0.4709      0.013     36.582      0.000       0.446
0.496
BMXBMI                0.3060      0.033      9.351      0.000       0.242
0.370
==============================================================================
Omnibus:                      752.325   Durbin-Watson:                   2.040
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1776.087
Skew:                           0.847   Prob(JB):                         0.00
Kurtosis:                       5.343   Cond. No.                         316.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
\end{Verbatim}

    Not surprisingly, BMI is positively associated with SBP. Given two
subjects with the same gender and age, and whose BMI differs by 1 unit,
the person with greater BMI will have, on average, 0.31 units greater
systolic blood pressure (SBP). Also note that after adding BMI to the
model, the coefficient for gender became somewhat greater. This is due
to the fact that the three covariates in the model, age, gender, and
BMI, are mutually correlated, as shown next:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{da}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, boxrule=.5pt, size=fbox, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{14}{\hspace{3.5pt}}
\begin{Verbatim}[commandchars=\\\{\}]
          RIDAGEYR  RIAGENDR    BMXBMI
RIDAGEYR  1.000000 -0.021398  0.023089
RIAGENDR -0.021398  1.000000  0.080463
BMXBMI    0.023089  0.080463  1.000000
\end{Verbatim}
\end{tcolorbox}
        
    Although the correlations among these three variables are not strong,
they are sufficient to induce fairly substantial differences in the
regression coefficients (e.g.~the gender coefficient changes from 3.23
to 3.58). In this example, the gender effect becomes larger after we
control for BMI - we can take this to mean that BMI was masking part of
the association between gender and blood pressure. In other settings,
including additional covariates can reduce the association between a
covariate and an outcome.

    \hypertarget{visualization-of-the-fitted-models}{%
\section{Visualization of the fitted
models}\label{visualization-of-the-fitted-models}}

    In this section we demonstrate some graphing techniques that can be used
to gain a better understanding of a regression model that has been fit
to data.

    We start with plots that allow us to visualize the fitted regression
function, that is, the mean systolic blood pressure expressed as a
function of the covariates. These plots help to show the estimated role
of one variable when the other variables are held fixed. We will also
plot 95\% \emph{simultaneous confidence bands} around these fitted
lines. The estimated mean curve is never exactly correct, but we can be
95\% confident that the true mean curve falls somewhere within the
shaded regions of the plots below.

    This type of plot requires us to fix the values of all variables other
than the independent variable (SBP here), and one independent variable
that we call the \emph{focus variable} (which is age here). Below we fix
the gender as ``female'' and the BMI as 25. Thus, the graphs below show
the relationship between expected SBP and age for women with BMI equal
to 25.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{sandbox}\PY{n+nn}{.}\PY{n+nn}{predict\PYZus{}functional} \PY{k}{import} \PY{n}{predict\PYZus{}functional}

\PY{c+c1}{\PYZsh{} Fix certain variables at reference values.  Not all of these}
\PY{c+c1}{\PYZsh{} variables are used here, but we provide them with a value anyway}
\PY{c+c1}{\PYZsh{} to prevent a warning message from appearing.}
\PY{n}{values} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Female}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{25}\PY{p}{,}
          \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DMDEDUC2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDRETH1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SMQ020}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}

\PY{n}{pr}\PY{p}{,} \PY{n}{cb}\PY{p}{,} \PY{n}{fv} \PY{o}{=} \PY{n}{predict\PYZus{}functional}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                \PY{n}{values}\PY{o}{=}\PY{n}{values}\PY{p}{,} \PY{n}{ci\PYZus{}method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{simultaneous}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{fv}\PY{p}{,} \PY{n}{pr}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fv}\PY{p}{,} \PY{n}{cb}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cb}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grey}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Age}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SBP}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_70_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The analogous plot for BMI is shown next. Here we fix the gender as
``female'' and the age at 50, so we are looking at the relationship
between expected SBP and age for women of age 50.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{del} \PY{n}{values}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{values}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{pr}\PY{p}{,} \PY{n}{cb}\PY{p}{,} \PY{n}{fv} \PY{o}{=} \PY{n}{predict\PYZus{}functional}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                \PY{n}{values}\PY{o}{=}\PY{n}{values}\PY{p}{,} \PY{n}{ci\PYZus{}method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{simultaneous}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{fv}\PY{p}{,} \PY{n}{pr}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fv}\PY{p}{,} \PY{n}{cb}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cb}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grey}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SBP}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_72_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The error band for BMI is notably wider than the error band for age,
indicating that there is less certainty about the relationsuip between
BMI and SBP compared to the relationship between age and SBP. This is
partly because the BMI values are less uniformly distributed. Very few
subjects have BMI greater than 50, so we have less information from the
data about the population mean, e.g. \(E[y|a=50, g]\) in this area.
Since there is less information, the uncertainty is greater, hence the
error bands are wider.

    \hypertarget{visualizing-variance-structure}{%
\subsection{Visualizing variance
structure}\label{visualizing-variance-structure}}

    The discussion so far has primarily focused on the mean structure of the
population, that is, the model for the average SBP of a person with a
given age, gender, and BMI. A regression model can also be used to
assess the \emph{variance structure} of the population -- that is, how
much and in what manner the observations deviate from their mean. We
will focus on informal, graphical methods for assessing this.

    To begin with, we plot the residuals against the fitted values. Recall
that the fitted values are the estimated means for each observation, and
the residuals are the difference between an observation and its fitted
mean. For example, the model may estimate that a 50 year old female will
have on average an SBP of 125. But a specific 50 year old female may
have a blood pressure of 110 or 150, for example. The fitted values for
both of these women are 125, and their residuals are -15, and 25,
respectively.

    The simplest variance pattern that we can see in a linear regression
occurs when the points are scattered around the mean, with the same
degree of scatter throughout the range of the covariates. This is called
\emph{homoescedasticity}, or \emph{constant variance}. When there are
multiple covariates, it is hard to assess whether the variance is
constant in this sense, but we can easily check for a ``mean/variance
relationship''. This means that there is a systematic relationship
between the variance and the mean, i.e.~the variance either increases or
decreases systematically with the mean. The plot of residuals on fitted
values is used to assess whether such a mean/variance relationship is
present.

    Below we show the plot of residuals on fitted values for the NHANES
data. It appears that we have a modestly increasing mean/variance
relationship. That is, the scatter around the mean blood pressure is
greater when the mean blood pressure itself is greater.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{pp} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{scatterplot}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{fittedvalues}\PY{p}{,} \PY{n}{result}\PY{o}{.}\PY{n}{resid}\PY{p}{)}
\PY{n}{pp}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fitted values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{pp}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuals}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_79_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    A ``component plus residual plot'' or ``partial residual plot'' is
intended to show how the data would look if all but one covariate could
be fixed at reference values. By controlling the values of these
covariates, all remaining variation is due either to the ``focus
variable'' (the one variable that is left unfixed, and is plotted on the
horizontal axis), or to sources of variation that are unexplained by any
of our covariates.

    For example, the partial residual plot below shows how age (horizontal
axis) and SBP (vertical axis) would be related if gender and BMI were
fixed. Note that the origin of the vertical axis in these plots is not
meaningful (we are not implying that anyone's blood pressure would be
negative), but the differences along the vertical axis are meaningful.
This plot implies that when BMI and gender are held fixed, the average
blood pressures of an 80 and 18 year old differ by around 30 mm/Hg. This
plot also shows, as discussed above, that the deviations from the mean
are somewhat smaller at the low end of the range compared to the high
end of the range. We also see that at the high end of the range, the
deviations from the mean are somewhat right-skewed, with exceptionally
high SBP values being more common than exceptionally low SBP values.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{regressionplots} \PY{k}{import} \PY{n}{plot\PYZus{}ccpr}

\PY{n}{plt}\PY{o}{.}\PY{n}{clf}\PY{p}{(}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{axes}\PY{p}{(}\PY{p}{)}
\PY{n}{plot\PYZus{}ccpr}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ax}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}alpha}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)} \PY{c+c1}{\PYZsh{} Reduce overplotting with transparency}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}color}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_82_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Next we have a partial residual plot that shows how BMI (horizontal
axis) and SBP (vertical axis) would be related if gender and age were
fixed. Compared to the plot above, we see here that age is more
uniformly distributed than BMI. Also, it appears that there is more
scatter in the partial residuals for BMI compared to what we saw above
for age. Thus there seems to be less information about SBP in BMI,
although a trend certainly exists.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{axes}\PY{p}{(}\PY{p}{)}
\PY{n}{plot\PYZus{}ccpr}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ax}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}alpha}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}color}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_84_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{assessing-curvature}{%
\section{Assessing curvature}\label{assessing-curvature}}

    As noted above, a linear model is ``linear'' in an important technical
sense, but this does not mean that linear regression can only be used to
model linear phenomena. The models that we fit above are all linear in
terms of the specified mean structure relationship between the
covariates (age, etc.) and the response (blood pressure). To assess
whether the data support this linear relationship, we can use an
\emph{added variable plot}. This is a diagnostic plot that aims to show
how the response is related to one of the predictors, independently of
how this relationship is specified in the model. Thus, although we have
modeled the mean structure relationship between age (for example) and
blood pressure as linear, the added variable plot will allow us to see
if the actual relationship deviates from this form.

    The cell below shows the added variable plot for age in a model
including main effects for age, BMI, and gender. Note that we are using
the GLM function here to fit the model. GLM is a more general class of
regression procedures that includes linear regression (OLS) as a special
case. OLS is the default for GLM, so the code below fits the identical
model as fit above with the OLS function. Currently, it is necessary to
use GLM when fitting linear models if we want to produce added variable
plots.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{regressionplots} \PY{k}{import} \PY{n}{add\PYZus{}lowess}

\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{GLM}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR + BMXBMI + RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{result}\PY{o}{.}\PY{n}{plot\PYZus{}added\PYZus{}variable}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}alpha}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{add\PYZus{}lowess}\PY{p}{(}\PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{frac}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_88_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The plot above suggests that the increasing trend identified by the
linear model is approximately correct, but that the true relationship
may be slightly nonlinear, with positive curvature. This means that the
annual mean increment in blood pressure may get larger as people age.

    We won't get into the mathematical details of how added variable plots
are constructed, but we note that the construction involves
residualization. Therefore, the ``focus variable'' (age above) is
centered relative to its mean in the plot.

    As another example, below is the added variable plot for BMI:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{regressionplots} \PY{k}{import} \PY{n}{add\PYZus{}lowess}

\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{GLM}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR + BMXBMI + RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{n}{fig} \PY{o}{=} \PY{n}{result}\PY{o}{.}\PY{n}{plot\PYZus{}added\PYZus{}variable}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{lines}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}alpha}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{add\PYZus{}lowess}\PY{p}{(}\PY{n}{fig}\PY{o}{.}\PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{frac}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_92_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{basis-functions-and-splines}{%
\section{Basis functions and
splines}\label{basis-functions-and-splines}}

    If we have identified possible nonlinear relationships that we would
like to include in a model, one way to accomplish this while remaining
within the framework of linear regression is to use basis functions. A
collection of basis functions is any collection of functions that we can
evaluate using one or more of the covariates in a model. For example, we
may consider the polynomial basis functions \(x, x^2, \ldots\). We can
evaluate these polynomials using age, yielding
\({\rm age}, {\rm age}^2, \ldots\). These variables can then be included
in the mean structure of the regression model:

    \[
E[{\rm SBP} \;|\; {\rm age}, {\rm BMI}] = b_0 + b_1\cdot {\rm age} + b_2\cdot{\rm age}^2 + b_3\cdot {\rm BMI}.
\]

    Note that this is still a linear model, since we can treat
\({\rm age}^2\) as simply being another covariate (like BMI) that we
have added to the model. Thus, all of the techniques used for working
with linear models remain applicable here, although we are now able to
capture nonlinear relationships in the mean structure. Also note that
above we included a polynomial (quadratic) term for age, but it would be
possible to simultaneously add polynomial terms for several terms in the
model (e.g.~for both age and BMI).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{da}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR\PYZus{}z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{da}\PY{o}{.}\PY{n}{RIDAGEYR} \PY{o}{\PYZhy{}} \PY{n}{da}\PY{o}{.}\PY{n}{RIDAGEYR}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{da}\PY{o}{.}\PY{n}{RIDAGEYR}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
\PY{n}{fml} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR\PYZus{}z + I(RIDAGEYR\PYZus{}z**2) + RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{n}{fml}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.215
Model:                            OLS   Adj. R-squared:                  0.214
Method:                 Least Squares   F-statistic:                     465.1
Date:                Mon, 27 Jan 2020   Prob (F-statistic):          4.21e-267
Time:                        15:01:25   Log-Likelihood:                -21505.
No. Observations:                5102   AIC:                         4.302e+04
Df Residuals:                    5098   BIC:                         4.304e+04
Df Model:                           3
Covariance Type:            nonrobust
================================================================================
======
                         coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
------
Intercept            123.8813      0.404    306.776      0.000     123.090
124.673
RIAGENDRx[T.Male]      3.2319      0.459      7.039      0.000       2.332
4.132
RIDAGEYR\_z             8.3594      0.231     36.185      0.000       7.907
8.812
I(RIDAGEYR\_z ** 2)     0.1859      0.248      0.750      0.453      -0.300
0.672
==============================================================================
Omnibus:                      708.996   Durbin-Watson:                   2.036
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1594.809
Skew:                           0.819   Prob(JB):                         0.00
Kurtosis:                       5.196   Cond. No.                         3.93
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
\end{Verbatim}

    As seen above, the coefficient for \({\rm age}^2\) is not significantly
different from zero. This is not the only arbiter of whether there is
curvature in the mean relationship between blood pressure and age, but
it does indicate that we are unable to estimate any curvature in these
data.

    Polynomial basis function are easy to explain, but have been criticized
for being too ``global'' and for inducing collinearity. The term
``global'' in this context means that the fitted value at one point
\(x\) in the covariate space may depend in unexpected ways on
observations \((y^\prime, x^\prime)\) for points \(x^\prime\) that are
far from \(x\). A very popular family of basis functions that avoid this
issue are the various \emph{spline} families, which are local
polynomials.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} bs(RIDAGEYR, 5) + BMXBMI + RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.231
Model:                            OLS   Adj. R-squared:                  0.230
Method:                 Least Squares   F-statistic:                     219.0
Date:                Mon, 27 Jan 2020   Prob (F-statistic):          2.76e-285
Time:                        15:01:25   Log-Likelihood:                -21451.
No. Observations:                5102   AIC:                         4.292e+04
Df Residuals:                    5094   BIC:                         4.297e+04
Df Model:                           7
Covariance Type:            nonrobust
================================================================================
======
                         coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
------
Intercept            103.6601      1.536     67.468      0.000     100.648
106.672
RIAGENDRx[T.Male]      3.6039      0.456      7.900      0.000       2.710
4.498
bs(RIDAGEYR, 5)[0]    -0.5552      2.268     -0.245      0.807      -5.002
3.891
bs(RIDAGEYR, 5)[1]     3.0907      1.575      1.962      0.050       0.002
6.179
bs(RIDAGEYR, 5)[2]    19.1714      2.124      9.024      0.000      15.007
23.336
bs(RIDAGEYR, 5)[3]    17.7960      1.762     10.099      0.000      14.341
21.251
bs(RIDAGEYR, 5)[4]    26.8944      1.498     17.955      0.000      23.958
29.831
BMXBMI                 0.3210      0.033      9.729      0.000       0.256
0.386
==============================================================================
Omnibus:                      745.462   Durbin-Watson:                   2.037
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1773.011
Skew:                           0.837   Prob(JB):                         0.00
Kurtosis:                       5.353   Cond. No.                         459.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
\end{Verbatim}

    Below we plot the fitted mean curve, which is an estimate of
\(E[{\rm SBP} | {\rm age}, {\rm gender}=F, {\rm BMI}=25]\), along with a
95\% simultaneous confidence band to assess the uncertainty.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{del} \PY{n}{da}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR\PYZus{}z}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{values} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Female}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{25}\PY{p}{,}
          \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DMDEDUC2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDRETH1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SMQ020}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}

\PY{n}{pr}\PY{p}{,} \PY{n}{cb}\PY{p}{,} \PY{n}{fv} \PY{o}{=} \PY{n}{predict\PYZus{}functional}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                \PY{n}{values}\PY{o}{=}\PY{n}{values}\PY{p}{,} \PY{n}{ci\PYZus{}method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{simultaneous}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{fv}\PY{p}{,} \PY{n}{pr}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{fv}\PY{p}{,} \PY{n}{cb}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{cb}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grey}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SBP}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_102_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{interactions}{%
\section{Interactions}\label{interactions}}

    The basic linear models we fit above are \emph{additive}, meaning that
the change in the expected response associated with a change in one
covariate is not dependent on the values of the other covariates. In the
setting of a regression model with two covariates, additivity implies
that the value of

\[
E[y | x_1=a+1, x_2=x] - E[y | x_1=a, x_2=x]
\]

does not depend on \(x\). In the blood pressure example, additivity
would imply that the difference in mean blood pressure between a 50
year-old woman and a 40 year-old woman is the same as the difference in
mean blood pressure between a 50 year-old man and a 40 year-old man. In
fact, this may not be the case.

    We now return to modeling the SBP as predicted by age and BMI, including
here an \emph{interaction} between age and gender in the model. This
interaction allows us to explore non-additivity between these variables.
Note that while nonlinearities and interactions can be considered
jointly, for simplicity we have removed the nonlinear part of the model
(the basis splines) so we can focus only on the interaction.

    Note that here we have centered the age variable. While not strictly
required, there are several advantages to centering quantitative
variables that are part of an interaction term in a regression model.
One specific advantage is that the main effects for the variables in the
interaction (here age and gender) are interpretable as the effects of
one of these variable while holding the other variable fixed at its
mean. For example, the main effect for gender in the model below is the
difference in mean blood pressure between women and men who are at the
mean age of the sample.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{da}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR\PYZus{}cen}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{da}\PY{o}{.}\PY{n}{RIDAGEYR} \PY{o}{\PYZhy{}} \PY{n}{da}\PY{o}{.}\PY{n}{RIDAGEYR}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{o}{.}\PY{n}{from\PYZus{}formula}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BPXSY1 \PYZti{} RIDAGEYR\PYZus{}cen*RIAGENDRx + BMXBMI}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{da}\PY{p}{)}
\PY{n}{result} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{result}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.239
Model:                            OLS   Adj. R-squared:                  0.238
Method:                 Least Squares   F-statistic:                     400.0
Date:                Mon, 27 Jan 2020   Prob (F-statistic):          4.37e-300
Time:                        15:01:26   Log-Likelihood:                -21425.
No. Observations:                5102   AIC:                         4.286e+04
Df Residuals:                    5097   BIC:                         4.289e+04
Df Model:                           4
Covariance Type:            nonrobust
================================================================================
==================
                                     coef    std err          t      P>|t|
[0.025      0.975]
--------------------------------------------------------------------------------
------------------
Intercept                        115.0502      1.026    112.149      0.000
113.039     117.061
RIAGENDRx[T.Male]                  3.5785      0.454      7.889      0.000
2.689       4.468
RIDAGEYR\_cen                       0.5766      0.018     32.394      0.000
0.542       0.611
RIDAGEYR\_cen:RIAGENDRx[T.Male]    -0.2182      0.026     -8.533      0.000
-0.268      -0.168
BMXBMI                             0.3013      0.032      9.273      0.000
0.238       0.365
==============================================================================
Omnibus:                      773.113   Durbin-Watson:                   2.040
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1861.274
Skew:                           0.861   Prob(JB):                         0.00
Kurtosis:                       5.406   Cond. No.                         139.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
\end{Verbatim}

    The results shown above show that the main effects and interactions are
all statistically significant, which suggests that the roles of age and
gender with respect to blood pressure may not be additive. Men have a
higher intercept but lower slope than women. The easiest way to see what
this tells us is through a graph:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{values}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Female}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{values}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{nan}
\PY{n}{pr1}\PY{p}{,} \PY{n}{cb1}\PY{p}{,} \PY{n}{fv1} \PY{o}{=} \PY{n}{predict\PYZus{}functional}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR\PYZus{}cen}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                \PY{n}{values}\PY{o}{=}\PY{n}{values}\PY{p}{,} \PY{n}{ci\PYZus{}method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{simultaneous}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{sns}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{whitegrid}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{fv1}\PY{p}{,} \PY{n}{pr1}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Female}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{values}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIAGENDRx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Male}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{pr2}\PY{p}{,} \PY{n}{cb2}\PY{p}{,} \PY{n}{fv2} \PY{o}{=} \PY{n}{predict\PYZus{}functional}\PY{p}{(}\PY{n}{result}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR\PYZus{}cen}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                \PY{n}{values}\PY{o}{=}\PY{n}{values}\PY{p}{,} \PY{n}{ci\PYZus{}method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{simultaneous}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{lineplot}\PY{p}{(}\PY{n}{fv2}\PY{p}{,} \PY{n}{pr2}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Male}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RIDAGEYR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SBP}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{nhanes_ols_files/nhanes_ols_109_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
